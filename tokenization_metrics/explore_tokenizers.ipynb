{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizer_exploration_utils import (\n",
    "    analyze_UD_file,\n",
    "    get_meta_data_for_languages,\n",
    "    plot_set_continuation,\n",
    "    plot_continuation,\n",
    "    plot_fertility,\n",
    "    plot_proportion_continuation,\n",
    "    plot_proportion_unks,\n",
    "    plot_dist_length\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_dir = \"<path/to/mecab/etc/mecabrc>\"\n",
    "mecab_dic_dir = \"<path/to/mecab-ipadic-20070801>\"\n",
    "\n",
    "monolingual_tokenizers = {\n",
    "    \"ar\": AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv01\", do_lower_case=False),\n",
    "    \"en\": AutoTokenizer.from_pretrained(\"bert-base-cased\"), \n",
    "    \"fi\": AutoTokenizer.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\"),\n",
    "    \"id\": AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\", do_lower_case=True),\n",
    "    \"ja\": AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char\", \n",
    "            mecab_kwargs={\n",
    "                \"mecab_option\": f\"-r {mecab_dir} -d {mecab_dic_dir}\"\n",
    "            }),\n",
    "    \"ko\": AutoTokenizer.from_pretrained(\"snunlp/KR-BERT-char16424\"),\n",
    "    \"ru\": AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\"),\n",
    "    \"tr\": AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\"),\n",
    "    \"zh\": AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "}\n",
    "multilingual_tokenizers = {\n",
    "    \"mBERT\": AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define UD dictionary with corresponding datasets files\n",
    "In these dictionaries we will load the tokenized data.  \n",
    "First for each of the language's tokenizer: `language_ud_dict`  \n",
    "Second for the mBERT tokenizer: `mBERT_language_ud_dict`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes path structure like: 'data/ud-data/ar/UD_Arabic-NYUAD/ar_nyuad-ud-train.conllu'\n",
    "data_dir = \"data/ud-data\"\n",
    "languages = [\"ar\", \"en\", \"fi\", \"id\", \"ja\", \"ko\", \"ru\", \"tr\", \"zh\"]\n",
    "\n",
    "language_ud_dict = {}\n",
    "mBERT_ud_dict = {} \n",
    "for l in languages:\n",
    "    # find all dev and train files for given language\n",
    "    l_files = glob.glob(os.path.join(data_dir, l, \"*\", \"*dev.conllu\"))\n",
    "    l_files.extend(glob.glob(os.path.join(data_dir, l, \"*\", \"*train.conllu\")))\n",
    "    # add files to dictionaries\n",
    "    language_ud_dict[l] = {\"files\": l_files}\n",
    "    mBERT_ud_dict[l] = {\"files\": l_files}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loader for the UD datasets\n",
    "Here we loop through all languages and corresponding files and tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the UD data for the monolingual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_meta_data_for_languages(language_ud_dict, monolingual_tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the UD data for mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_meta_data_for_languages(mBERT_ud_dict, multilingual_tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UD Plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UD --- Proportion of Continued Words\n",
    "Proportion of words that are split at least into two tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuation_df = plot_proportion_continuation([language_ud_dict, mBERT_ud_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  UD --- Fertility\n",
    "Average number of tokens a single word was split into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fertility_df = plot_fertility([language_ud_dict, mBERT_ud_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UD --- UNK Proportion\n",
    "Proportion of tokens which are not represented in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_df = plot_proportion_unks([language_ud_dict, mBERT_ud_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UD --- Sentence length Plots \n",
    "Sentence length of actual sentence in the UD dataset vs sentence length when tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_dist_length(language_ud_dict, mBERT_ud_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
